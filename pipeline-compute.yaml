AWSTemplateFormatVersion: '2010-09-09'
Description: 'Document Pipeline - Compute Stack (Lambda)'

Parameters:
  Environment:
    Type: String
    Default: dev
    Description: Environment name
  ProjectId:
    Type: String
    Description: Project ID
  PortfolioId:
    Type: String
    Description: Portfolio ID
  StackName:
    Type: String
    Description: Name of the stack
  InputDocumentBucketName:
    Type: String
    Description: S3 Bucket Name for inputs
  JobStatusTableName:
    Type: String
    Description: DynamoDB Table Name for job status
  TextractJobCompleteTopicArn:
    Type: String
    Description: SNS Topic ARN for Textract notifications
  KmsKeyArn:
    Type: String
    Description: KMS Key ARN for encryption
  DocumentUploadQueueArn:
    Type: String
    Description: ARN of SQS Queue for document upload events
  TextractCompleteQueueArn:
    Type: String
    Description: ARN of SQS Queue for Textract completion events

Resources:
  PipelineLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: PipelineAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:PutObject
                Resource: 
                  - !Sub arn:aws:s3:::${InputDocumentBucketName}
                  - !Sub arn:aws:s3:::${InputDocumentBucketName}/*
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:GetItem
                Resource: !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${JobStatusTableName}
              - Effect: Allow
                Action:
                  - textract:StartDocumentAnalysis
                  - textract:GetDocumentAnalysis
                Resource: '*'
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref TextractJobCompleteTopicArn
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:GenerateDataKey
                Resource: !Ref KmsKeyArn
              - Effect: Allow
                Action:
                  - sqs:ReceiveMessage
                  - sqs:DeleteMessage
                  - sqs:GetQueueAttributes
              - Effect: Allow
                Action:
                  - sqs:ReceiveMessage
                  - sqs:DeleteMessage
                  - sqs:GetQueueAttributes
                Resource: 
                  - !Ref DocumentUploadQueueArn
                  - !Ref TextractCompleteQueueArn

  # --------------------------------------------------------------------------
  # 1. Start Textract Job (Triggered by S3 Event via EventBridge)
  # --------------------------------------------------------------------------
  StartTextractFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${ProjectId}-${PortfolioId}-${StackName}-${Environment}-start-textract
      Runtime: python3.11
      Handler: index.handler
      Role: !GetAtt PipelineLambdaRole.Arn
      Timeout: 60
      Environment:
        Variables:
          JOB_TABLE: !Ref JobStatusTableName
          SNS_TOPIC_ARN: !Ref TextractJobCompleteTopicArn
          KMS_KEY_ARN: !Ref KmsKeyArn
      Code:
        ZipFile: |
          import json
          import boto3
          import os

          textract = boto3.client('textract')
          dynamodb = boto3.table(os.environ['JOB_TABLE'])

          def handler(event, context):
              print("Received SQS event:", json.dumps(event))
              
              for record in event['Records']:
                  # SQS Body contains the S3 Event
                  s3_event = json.loads(record['body'])
                  
                  # Some test events might not have Records
                  if 'Records' not in s3_event:
                      print("Skipping non-S3 event body:", s3_event)
                      continue
                      
                  for s3_record in s3_event['Records']:
                      bucket = s3_record['s3']['bucket']['name']
                      key = s3_record['s3']['object']['key']

              print(f"Starting Textract for {bucket}/{key}")
              
              response = textract.start_document_analysis(
                  DocumentLocation={'S3Object': {'Bucket': bucket, 'Name': key}},
                  FeatureTypes=['TABLES', 'FORMS'],
                  NotificationChannel={
                      'SNSTopicArn': os.environ['SNS_TOPIC_ARN'],
                      'RoleArn': '...' # Requires an IAM role for Textract to publish to SNS. Passing RoleArn here implies Lambda creates it or we pass a pre-created one.
                      # Simpler: Use JobId to poll or waiting for SNS if permissions allow.
                      # Ideally: We need a Service Role for Textract to publish to SNS.
                  },
                  KMSKeyId=os.environ['KMS_KEY_ARN'] 
              )
              
              job_id = response['JobId']
              dynamodb.put_item(Item={'JobId': job_id, 'Status': 'STARTED', 'S3Key': key})
              return {'JobId': job_id}

  # Textract Service Role to publish to SNS (Required for StartDocumentAnalysis)
  TextractPublishRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: textract.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: SNSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: sns:Publish
                Resource: !Ref TextractJobCompleteTopicArn
  
  # Trigger rule for StartTextractFunction

  # Trigger Lambda from SQS
  StartTextractEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      BatchSize: 10
      EventSourceArn: !Ref DocumentUploadQueueArn
      FunctionName: !Ref StartTextractFunction
      Enabled: true

  # --------------------------------------------------------------------------
  # 2. Process Textract Results (Triggered by SNS)
  # --------------------------------------------------------------------------
  UpdateSalesForceFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${ProjectId}-${PortfolioId}-${StackName}-${Environment}-update-salesforce
      Runtime: python3.11
      Handler: index.handler
      Role: !GetAtt PipelineLambdaRole.Arn
      Timeout: 300
      Environment:
        Variables:
          JOB_TABLE: !Ref JobStatusTableName
      Code:
        ZipFile: |
          import json
          import boto3
          import os

          textract = boto3.client('textract')
          dynamodb = boto3.resource('dynamodb').Table(os.environ['JOB_TABLE'])

          def handler(event, context):
              print("Received Event:", json.dumps(event))
              
              if 'Records' in event:
                  for record in event['Records']:
                      # Case 1: SNS Direct Trigger
                      if 'Sns' in record:
                          print("Processing SNS Record")
                          message = json.loads(record['Sns']['Message'])
                          process_message(message)
                      
                      # Case 2: SQS Trigger (Body contains SNS Message)
                      elif 'body' in record:
                          print("Processing SQS Record")
                          sns_notification = json.loads(record['body'])
                          if 'Message' in sns_notification:
                              message = json.loads(sns_notification['Message'])
                              process_message(message)
                          else:
                              print("Unknown SQS Body format")
              
              return {'status': 'processed'}

          def process_message(message):
              job_id = message['JobId']
              status = message['Status']
              process_job(job_id, status)

          def process_job(job_id, status):
              print(f"Processing Job: {job_id}, Status: {status}")
              
              if status == 'SUCCEEDED':
                  print(f"Job {job_id} succeeded. Fetching results...")
                  # In real implementation: handle pagination
                  # response = textract.get_document_analysis(JobId=job_id)
                  # Placeholder: Send to Salesforce
                  print("Sending payload to Salesforce...")
                  
                  dynamodb.update_item(
                      Key={'JobId': job_id},
                      UpdateExpression="set Status=:s",
                      ExpressionAttributeValues={':s': 'COMPLETED'}
                  )
              else:
                  print(f"Job {job_id} failed.")
                  dynamodb.update_item(
                      Key={'JobId': job_id},
                      UpdateExpression="set Status=:s",
                      ExpressionAttributeValues={':s': 'FAILED'}
                  )

  ProcessTextractEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      BatchSize: 10
      EventSourceArn: !Ref TextractCompleteQueueArn
      FunctionName: !Ref UpdateSalesForceFunction
      Enabled: true

  ProcessTextractFunctionPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref UpdateSalesForceFunction
      Action: lambda:InvokeFunction
      Principal: sns.amazonaws.com
      SourceArn: !Ref TextractJobCompleteTopicArn

  TextractTopicSubscription:
    Type: AWS::SNS::Subscription
    Properties:
      Endpoint: !GetAtt UpdateSalesForceFunction.Arn
      Protocol: lambda
      TopicArn: !Ref TextractJobCompleteTopicArn

  # --------------------------------------------------------------------------
  # 3. Ingestion Function (Scheduled)
  # --------------------------------------------------------------------------
  ManualLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${ProjectId}-${PortfolioId}-${StackName}-${Environment}-manual
      Runtime: python3.11
      Handler: index.handler
      Role: !GetAtt PipelineLambdaRole.Arn
      Timeout: 300
      Environment:
        Variables:
          BUCKET_NAME: !Ref InputDocumentBucketName
      Code:
        ZipFile: |
          import json
          import boto3
          import os

          s3 = boto3.client('s3')

          def handler(event, context):
              print("Scheduled Ingestion Triggered")
              # Placeholder: Fetch from Parchment/NSC
              # Upload dummy PDF for testing
              bucket = os.environ['BUCKET_NAME']
              key = 'test-document.pdf'
              s3.put_object(Bucket=bucket, Key=key, Body=b'Dummy content')
              print(f"Uploaded {key} to {bucket}")
              return {'status': 'Ingestion Complete'}

  IngestionSchedule:
    Type: AWS::Events::Rule
    Properties:
      ScheduleExpression: rate(1 day)
      Targets:
        - Arn: !GetAtt ManualLambdaFunction.Arn
          Id: TargetFunctionV1

  IngestionFunctionPermission:
    Type: AWS::Lambda::Permission
    DependsOn:
      - IngestionSchedule
    Properties:
      FunctionName: !Ref ManualLambdaFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt IngestionSchedule.Arn

  ClearinghouseFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${ProjectId}-${PortfolioId}-${StackName}-${Environment}-clearinghouse

      Runtime: python3.11
      Handler: index.handler
      Role: !GetAtt PipelineLambdaRole.Arn
      Timeout: 300
      Environment:
        Variables:
          BUCKET_NAME: !Ref InputDocumentBucketName
      Code:
        ZipFile: |
          import json
          import boto3
          import os

          s3 = boto3.client('s3')

          def handler(event, context):
              print("Scheduled Ingestion Triggered")
              # Placeholder: Fetch from Parchment/NSC
              # Upload dummy PDF for testing
              bucket = os.environ['BUCKET_NAME']
              key = 'test-document.pdf'
              s3.put_object(Bucket=bucket, Key=key, Body=b'Dummy content')
              print(f"Uploaded {key} to {bucket}")
              return {'status': 'Ingestion Complete'}

  ClearinghouseSchedule:
    Type: AWS::Events::Rule
    Properties:
      ScheduleExpression: rate(1 day)
      Targets:
        - Arn: !GetAtt ClearinghouseFunction.Arn
          Id: TargetFunctionV1

  ClearinghouseFunctionPermission:
    Type: AWS::Lambda::Permission
    DependsOn:
      - ClearinghouseSchedule
    Properties:
      FunctionName: !Ref ClearinghouseFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt ClearinghouseSchedule.Arn

Outputs:
  StartTextractFunctionArn:
    Value: !GetAtt StartTextractFunction.Arn
  ProcessTextractFunctionArn:
    Value: !GetAtt UpdateSalesForceFunction.Arn
  ManualLambdaFunctionArn:
    Value: !GetAtt ManualLambdaFunction.Arn
  ClearinghouseFunctionArn:
    Value: !GetAtt ClearinghouseFunction.Arn
